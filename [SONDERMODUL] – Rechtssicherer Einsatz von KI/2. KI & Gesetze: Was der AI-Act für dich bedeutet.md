Stell dir vor, auf Social Media geht ein Video viral.  
Darauf zu sehen: ein bekannter Politiker, der behauptet, die Erde sei flach.  

Klingt nach einem dummen Scherz.  
Und das ist es auch – denn das Video wurde mit KI erstellt.  

Das interessiert die Zuschauer allerdings nur wenig.  
Viele bemerken gar nicht, dass es sich dabei um ein KI-generiertes Video handelt.  

Die Vorstellung, jemand könnte ein ähnliches Video von einem selbst veröffentlichen, ist gruselig.  
Aber dieses Gefühl ist nachvollziehbar.  

Da gibt es plötzlich ein neues Tool, das uns so viele Möglichkeiten verspricht  
und quasi jedem zur Verfügung steht.  
Niemand kann aber genau sagen, wohin die Reise geht,  
was alles möglich sein wird  
und was Menschen mit diesen Möglichkeiten anfangen.  

Umso dringender braucht es klare gesetzliche Rahmen,  
die Sicherheit und Orientierung geben.  

Die EU hat hier eine Vorreiterrolle übernommen  
und den sogenannten AI Act geschaffen.  

Er ist sozusagen eine Sammlung von Richtlinien und Regulierungen,  
die EU-weit für KI-Anwender und -Anbieter gelten sollen.  
Ziel ist es, einen neuen Sicherheitsstandard zu schaffen,  
der sowohl Anbieter als auch Anwender schützt.  

KI-Anwendungen werden im AI Act in vier Risikogruppen eingeteilt.  
Diese Einteilung basiert auf dem potenziellen Schaden,  
den eine KI-Anwendung anrichten kann.  

Da wären zunächst die **inakzeptablen Risiken**.  
Sie betreffen Anwendungen, deren Auswirkungen so groß und unvorhersehbar sind,  
dass wir sie schlichtweg nicht akzeptieren können.  

Ein Beispiel ist die biometrische Gesichtserkennung im öffentlichen Raum.  
Sie lässt sich nicht mit dem Persönlichkeitsrecht vereinbaren.  
Solche Anwendungen werden durch den AI Act grundsätzlich verboten.  

Dann gibt es die **hohen Risiken**,  
auch **Hochrisiko-KI-Systeme** genannt.  
Diese können eine potenzielle Bedrohung für Sicherheit, Gesundheit  
oder Grundrechte darstellen – etwa Anwendungen zur Strafverfolgung.  

Hier gilt:  
Solche Systeme müssen mit besonderer Vorsicht eingesetzt  
und durch klare Sicherheitskonzepte begleitet werden.  
Dazu gehören zum Beispiel KI-gestützte medizinische Geräte,  
KI-Systeme, die kritische Infrastruktur beeinflussen,  
oder eben die genannten Systeme zur Strafverfolgung.  

Die dritte Gruppe sind **begrenzte Risiken**.  
Sie entstehen, wenn ein Nutzer nicht weiß,  
dass er mit einer KI interagiert – zum Beispiel bei Chatbots.  

Gegenüber einem Menschen würden sich viele Nutzer vermutlich anders verhalten  
als gegenüber einer KI.  
Und den Rat einer KI würden sie vielleicht anders bewerten als den eines Menschen.  

Hier fordert der AI Act Transparenz.  
Nutzer müssen künftig klar darauf hingewiesen werden,  
dass sie mit einem KI-System interagieren.  
Dazu aber später mehr.  

Zuletzt gibt es noch die **minimalen oder geringen Risiken**.  
Dazu gehören die meisten Anwendungen, die bereits heute in der EU eingesetzt werden –  
etwa KI-Suchmaschinen, Spamfilter oder auch KI in Videospielen.  

Systeme mit geringem Risiko unterliegen deutlich weniger Regulierungen.  
Trotzdem müssen auch sie nachweislich sicher sein.
Deshalb empfiehlt die EU, sich an den Standards für Hochrisiko-Systeme zu orientieren.
Denn auch wenn die Risiken gering erscheinen – Die Verantwortung bleibt immer beim Menschen.  
